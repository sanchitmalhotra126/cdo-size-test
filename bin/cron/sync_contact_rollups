#!/usr/bin/env ruby

require File.expand_path('../../../pegasus/src/env', __FILE__)
require src_dir 'database'
require_relative('../../dashboard/config/environment')
require 'cdo/properties'

# Connection to read from reporting database.
PEGASUS_REPORTING_DB_READER = sequel_connect(CDO.pegasus_reporting_db_reader, CDO.pegasus_reporting_db_reader)

# Connection to read from production database.
PEGASUS_DB_READER = sequel_connect(CDO.pegasus_db_writer, CDO.pegasus_db_reader)

# Connection to write to production database. (We use a 2nd production database connection because we need to do writes
# while continuing to iterate through results of an earlier read query from that same database.)
PEGASUS_DB_WRITER = sequel_connect(CDO.pegasus_db_writer, CDO.pegasus_db_reader)

# Columns to disregard
EXCLUDED_COLUMNS = %w(id pardot_id pardot_sync_at updated_at).freeze

def log(s)
  puts s      # emit to stdout
  CDO.log.info s   # emit to log file
end

# helper function to get next record in SQL result set iterator
def grab_next(s)
  s.next
rescue StopIteration
  nil
end

def main
  log("#{Time.now} Starting")

  num_inserts = 0
  num_updates = 0
  num_unchanged = 0

  # We use raw SQL in the queries below in order to include FORCE INDEX, which turns out to be necessary to make
  # MySQL use the email index to sort by email.

  # Query all of the contacts in the latest daily contact rollup table (contact_rollups_daily) sorted by email.
  contact_rollups_src = PEGASUS_REPORTING_DB_READER['SELECT * FROM contact_rollups_daily FORCE INDEX(contact_rollups_email_index) ORDER BY email']
  # Query all of the contacts in the master contact rollup table (contact_rollups_daily) sorted by email.
  contact_rollups_dest = PEGASUS_DB_READER['SELECT * FROM contact_rollups FORCE INDEX(contact_rollups_email_index) ORDER BY email']

  # Create iterators for both queries using the #stream method so we stream the results back rather than
  # trying to load everything in memory
  src_iterator = contact_rollups_src.stream.to_enum
  dest_iterator = contact_rollups_dest.stream.to_enum

  # Do a row-by-row comparison of the new daily contact rollup (src) against the existing (dest) and calculate
  # the differences. Make inserts or updates to make dest match src. Wherever we do an insert or an update,
  # mark the updated_at timestamp so we track what rows have changed at what time, to enable efficient sync
  # into Pardot.

  contact_rollup_src = grab_next(src_iterator)
  contact_rollup_dest = grab_next(dest_iterator)
  until contact_rollup_src.nil?

    email_src = contact_rollup_src[:email]

    # Continue to advance the destination pointer until the destination email address in question
    # is the same or later alphabetically as the source email address.
    while (!contact_rollup_dest.nil?) &&
        ((contact_rollup_dest[:email] <=> email_src) == -1)
      contact_rollup_dest = grab_next(dest_iterator)
    end

    # Determine if this is a new record (email address doesn't exist in destination table) or existing.
    is_new = true
    unless contact_rollup_dest.nil?
      email_dest = contact_rollup_dest[:email]
      # Compare email addresses of source and destination row to see if they are the same.
      # IF not, the source record is new.
      is_new = (email_src.casecmp(email_dest) != 0)
    end

    # Build the set of column data that needs to be inserted/updated
    output_row = {}
    contact_rollup_src.each do |column, value|
      # Skip columns that include sync metadata
      next if EXCLUDED_COLUMNS.include? column.to_s

      # If this is not a new record, skip columns where the data already matches
      next if !is_new && (value == contact_rollup_dest[column])

      # Add the source data to the same column in the destination row
      output_row[column] = value
    end

    # If there are no columns to update, then this is an existing record where the
    # destination is the same as the source, nothing to do
    if output_row.empty?
      num_unchanged += 1
    else

      # Track the timestamp of the change, so the sync process knows this change needs to be synced
      output_row[:updated_at] = Time.now

      if is_new
        # Insert the destination record
        log("#{Time.now} Inserted #{email_src} (src id: #{contact_rollup_src[:id]})")
        PEGASUS_DB_WRITER[:contact_rollups].insert(output_row)
        num_inserts += 1
      else
        # Update the destination record
        log("#{Time.now} Update #{email_src} (src id: #{contact_rollup_src[:id]}; updated: #{output_row})")
        PEGASUS_DB_WRITER[:contact_rollups].where(email: email_src).update(output_row)
        num_updates += 1
      end
    end

    # Go on to the next source record
    contact_rollup_src = grab_next(src_iterator)
  end

  num_total = num_inserts + num_updates + num_unchanged
  log("#{Time.now} Completed. #{num_total} source rows processed. #{num_inserts} insert(s), #{num_updates} update(s), #{num_unchanged} unchanged.")
end

main
