#!/usr/bin/env ruby

# This script fetches covid19 data and uploads to the shared firebase channel.
require_relative '../../../deployment'
require 'cdo/only_one'
require 'net/http'
require 'csv'
require 'date'
require 'datapackage'
require_relative '../../../shared/middleware/helpers/firebase_helper'

US_DATA_URL = "https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv"
MAX_DAYS = 90 # there are 55 states/territories, and the table must be less than 5000 total rows (5000/55 = 90.9)
MAX_WEEKS = 26 # there are 188 countries, and the table must be less than 5000 total rows (5000/188 = 26.5)

def get_us_covid19_data
  response = Net::HTTP.get_response(URI(US_DATA_URL))
  return unless response.code == '200'
  response.body.force_encoding('utf-8')
  lines = CSV.parse(response.body, headers: true)

  # Group records by date
  daily_data = {}
  lines.each do |line|
    record = {}
    parsed_date = Date.strptime(line.field("date"), "%Y-%m-%d")
    formatted_date = parsed_date.strftime('%a %-m/%-d') # ex Wed 1/7
    record['Date'] = formatted_date
    record['State'] = line.field("state")
    record['Confirmed New Cases'] = line.field("cases").to_i
    record['Deaths'] = line.field("deaths").to_i
    daily_data[formatted_date] ||= []
    daily_data[formatted_date].push record
  end

  # Truncate to MAX_DAYS most recent days
  days = if daily_data.length > MAX_DAYS
           daily_data.keys.slice(daily_data.length - MAX_DAYS, MAX_DAYS)
         else
           daily_data.keys
         end

  # Add id to each record
  columns = ['id', 'Date', 'State', 'Confirmed New Cases', 'Deaths']
  records = {}
  id = 1
  days.each do |day|
    day_records = daily_data[day]
    day_records.each do |record|
      record['id'] = id
      records[id] = record.to_json
      id += 1
    end
  end

  return records, columns
end

def get_world_covid19_data
  package = DataPackage::Package.new('https://datahub.io/core/covid-19/datapackage.json')
  resource = package.resources.find {|r| r['name'] == "countries-aggregated_csv"}
  return unless resource
  response = Net::HTTP.get_response(URI(resource['path']))
  return unless response.code == '200'
  lines = CSV.parse(response.body, headers: true)
  weekly_data = {}

  # Aggregate daily data by week
  lines.each do |line|
    parsed_date = Date.strptime(line.field("Date"), "%Y-%m-%d")
    weekly_data[parsed_date.cweek] ||= {}
    weekly_data[parsed_date.cweek][line.field("Country")] ||= {confirmed: 0, recovered: 0, deaths: 0}
    country = weekly_data[parsed_date.cweek][line.field("Country")]
    country[:confirmed] += line.field("Confirmed").to_i
    country[:recovered] += line.field("Recovered").to_i
    country[:deaths] += line.field("Deaths").to_i
  end

  # Truncate to MAX_WEEKS most recent weeks
  weeks = if weekly_data.length > MAX_WEEKS
            weekly_data.keys.slice(weekly_data.length - MAX_WEEKS, MAX_WEEKS)
          else
            weekly_data.keys
          end

  columns = ['id', 'Date', 'Country', 'Confirmed New Cases', 'Recovered', 'Deaths']
  records = {}
  id = 1
  weeks.each do |week_number|
    date = Date.commercial(2020, week_number)
    countries = weekly_data[week_number]
    countries.each do |country, counts|
      record = {}
      record['id'] = id
      record['Date'] = date.strftime('%a %-m/%-d') # ex Wed 1/7
      record['Country'] = country
      record['Confirmed New Cases'] = counts[:confirmed]
      record['Recovered'] = counts[:recovered]
      record['Deaths'] = counts[:deaths]

      records[id] = record.to_json
      id += 1
    end
  end
  return records, columns
end

def main
  fb = FirebaseHelper.new('shared')
  records, columns = get_us_covid19_data
  fb.upload_live_table('COVID-19 Cases per US State', records, columns)

  records, columns = get_world_covid19_data
  fb.upload_live_table('COVID-19 Cases per Country', records, columns)
end

main if only_one_running?(__FILE__)
