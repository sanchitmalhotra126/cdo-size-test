<%
require 'cdo/aws/dms'
require 'active_support/core_ext/numeric/bytes'
require 'active_support/core_ext/numeric/time'
-%>
---
AWSTemplateFormatVersion: 2010-09-09
Description: Data layer for Tableau including RedShift cluster configuration and synchronization with RDS instance.
# Parameters can be provided via CDO.underscored_parameter, e.g. via locals.yml:
# redshift_password: abcdef
# Parameters are only required for initial stack creation, and reused if not provided on stack update.
Parameters:
  RDSIdentifier:
    Type: String
  # MySQL user must have granted REPLICATION CLIENT and REPLICATION SLAVE permissions.
  # Ref: http://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.MySQL.html#CHAP_Source.MySQL.Security
  RDSUsername:
    Type: String
  RDSPassword:
    Type: String
    NoEcho: true
  RDSHost:
    Type: String
  RedshiftDatabase:
    Type: String
    Default: dashboard
  RedshiftUsername:
    Type: String
    Default: dev
  RedshiftPassword:
    Type: String
    NoEcho: true
Resources:
  Tableau:
    Type: AWS::Redshift::Cluster
    DeletionPolicy: Retain
    Properties:
      AllowVersionUpgrade: true
      AutomatedSnapshotRetentionPeriod: 1
      ClusterParameterGroupName: default.redshift-1.0
      ClusterSubnetGroupName: !ImportValue VPC-RedshiftSubnetGroup
      ClusterType: single-node
      ClusterVersion: 1.0
      DBName: {Ref: RedshiftDatabase}
      Encrypted: true
      KmsKeyId: alias/aws/redshift
      MasterUsername: {Ref: RedshiftUsername}
      MasterUserPassword: {Ref: RedshiftPassword}
      NodeType: dc1.large
      PubliclyAccessible: true
      VpcSecurityGroupIds: [!ImportValue VPC-RedshiftSecurityGroup]
  TableauSync:
    Type: AWS::DMS::ReplicationInstance
    Properties:
      AllocatedStorage: 250
      EngineVersion: 2.4.0
      MultiAZ: true
      PubliclyAccessible: false
      ReplicationInstanceClass: dms.c4.4xlarge
      ReplicationInstanceIdentifier: !Ref AWS::StackName
      ReplicationSubnetGroupIdentifier: !ImportValue VPC-DMSSubnetGroup
      VpcSecurityGroupIds: [!ImportValue VPC-DMSSecurityGroup]
  RDSEndpoint:
    Type: AWS::DMS::Endpoint
    Properties:
      EndpointIdentifier: !Sub "RDS-${AWS::StackName}"
      EndpointType: source
      EngineName: mysql
      ServerName: !Ref RDSHost
      Port: 3306
      Username: !Ref RDSUsername
      Password: !Ref RDSPassword
  RedshiftEndpoint:
    Type: AWS::DMS::Endpoint
    Properties:
      EndpointIdentifier: !Sub "Redshift-${AWS::StackName}"
      EndpointType: target
      EngineName: redshift
      ServerName: !GetAtt Tableau.Endpoint.Address
      Port: !GetAtt Tableau.Endpoint.Port
      Username: !Ref RedshiftUsername
      Password: !Ref RedshiftPassword
      DatabaseName: !Ref RedshiftDatabase
      # Extra Connection Attributes:
      # https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Introduction.ConnectionAttributes.html#CHAP_Introduction.ConnectionAttributes.Redshift
      #
      # maxFileSize - Specifies the maximum size (in KB) of any CSV file used to transfer data to Amazon Redshift.
      # Default value: 32768 KB (32 MB)
      # Valid values: 1 - 1048576
      #
      # fileTransferUploadStreams - Specifies the number of threads used to upload a single file.
      # Default value: 10
      # Valid values: 1 - 64
      ExtraConnectionAttributes: <%={
         maxFileSize: 100.megabytes / 1.kilobyte,
         fileTransferUploadStreams: 20
      }.map{|x|x.join('=')}.join(';')%>
<% Cdo::DMS.tasks(aws_dir('dms/tasks.yml')).each do |task_name, table_mappings| -%>
  DMS<%=task_name.underscore.camelize%>:
    Type: AWS::DMS::ReplicationTask
    Properties:
      MigrationType: full-load-and-cdc
      ReplicationInstanceArn: !Ref TableauSync
      ReplicationTaskIdentifier: <%= task_name %>
      SourceEndpointArn: !Ref RDSEndpoint
      TargetEndpointArn: !Ref RedshiftEndpoint
      ReplicationTaskSettings: !Sub |
        <%= YAML.load(erb_file(aws_dir('dms/replication-task-settings.yml.erb'))).to_json %>
      TableMappings: !Sub |
        <%= table_mappings.to_json %>
<% end -%>
  ReportingDBParameters:
    Type: AWS::RDS::DBParameterGroup
    Properties:
      Description: !Sub "Reporting Read Replica DB Parameters for ${AWS::StackName}."
      Family: mysql5.7
      Parameters:
        # Allow redo log to grow larger before flushing for better write I/O efficiency.
        innodb_adaptive_flushing_lwm: 40
        # Improve insert/update concurrency for autoincrement tables.
        innodb_autoinc_lock_mode: 2
        # Reduce durability for better performance.
        innodb_flush_log_at_trx_commit: 0
        # Neighbor flushing reduces performance without any benefit on SSD.
        innodb_flush_neighbors: 0
        # IO capacity controls background flushing rate (IO/sec).
        innodb_io_capacity: 1000
        # IO capacity max controls maximum background flushing rate when certain thresholds are reached.
        innodb_io_capacity_max: 3000
        # Larger log buffer size allows larger transactions without writing to disk.
        innodb_log_buffer_size: <%=16.megabytes%>
        # Larger redo log file size allows write combining for better write I/O efficiency.
        innodb_log_file_size: <%=2.gigabytes%>
        # LRU scan depth controls LRU flushing rate (IO/sec).
        innodb_lru_scan_depth: 1000
        # Enable all InnoDB performance schema monitors for better debugging.
        innodb_monitor_enable: all
        # Random read ahead can improve overall read-query performance, important for reporting.
        innodb_random_read_ahead: 1
        # Increase number of threads for background reads (read-ahead)
        innodb_read_io_threads: 8
        # Increase sample pages for more accurate statistics for query plans.
        innodb_stats_persistent_sample_pages: 60
        # 'Splits an internal data structure used to coordinate threads, for higher concurrency in workloads with large numbers of waiting threads.'
        # AWS recommended increasing to max (1024).
        innodb_sync_array_size: 1024
        # Number of threads for background writes
        innodb_write_io_threads: 8
        # Reporting DB currently needs write access for contact rollup staging table.
        read_only: 0
        # Don't resolve DNS on usernames.
        skip_name_resolve: 1
        # Use compression for replication.
        slave_compressed_protocol: 1
        # Enable parallel replication.
        slave_parallel_workers: 8
        # Enable parallel replication without violating consistency.
        slave_parallel_type: LOGICAL_CLOCK
        slave_preserve_commit_order: 1
        # Enable slow query log.
        slow_query_log: 1
        # Reduce durability for better performance.
        sync_binlog: 0
        # Fully cache all open tables and table definitions.
        table_definition_cache: 20000
        table_open_cache: 20000
        # Relax transaction isolation for improved concurrency under load.
        tx_isolation: READ-COMMITTED
        # Requirements for DMS Change Data Capture.
        # Ref: https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.MySQL.html#CHAP_Source.MySQL.AmazonManaged
        binlog_format: ROW
        binlog_checksum: NONE
<%
  # Add CloudWatch Logs Metric Filters for RDS Enhanced Monitoring metrics.
  # Ref: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html#w2ab1c20c23c17b7b5
  %w(
    diskIO.writeKbPS
    diskIO.readIOsPS
    diskIO.await
    diskIO.readKbPS
    diskIO.rrqmPS
    diskIO.util
    diskIO.avgQueueLen
    diskIO.tps
    diskIO.readKb
    diskIO.writeKb
    diskIO.avgReqSz
    diskIO.wrqmPS
    diskIO.writeIOsPS
    cpuUtilization.total
    cpuUtilization.guest
    cpuUtilization.irq
    cpuUtilization.system
    cpuUtilization.wait
    cpuUtilization.idle
    cpuUtilization.user
    cpuUtilization.total
    cpuUtilization.steal
    cpuUtilization.nice
    tasks.total
    tasks.running
    tasks.blocked
  ).each do |metric|
-%>
  <%=metric.gsub('.', '_').underscore.camelize%>:
    Type: AWS::Logs::MetricFilter
    Properties:
      LogGroupName: RDSOSMetrics
      FilterPattern: "{ $.instanceID = production }"
      MetricTransformations:
        - MetricValue: "$.<%=metric.sub('diskIO', 'diskIO[0]')%>"
          MetricNamespace: "MySQL"
          MetricName: <%=metric.capitalize%>
<% end -%>
